{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNA Data Storage\n",
    "Recovering the data stored on DNA by clustering the noisy reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from Bio.Align.Applications import MuscleCommandline\n",
    "from Bio import AlignIO\n",
    "from skbio.alignment import local_pairwise_align_ssw\n",
    "from skbio import DNA\n",
    "import itertools\n",
    "import operator\n",
    "import time\n",
    "import zipfile\n",
    "import subprocess as sp\n",
    "import hashlib\n",
    "from include import *\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "You may choose to work with either a simulated dataset or a real synthesized dataset. The overall steps you need to take for each data are as follows:<br> <br>\n",
    "Simulated data: <br>\n",
    "In this example, the following stages are tested: <br>\n",
    "1- Loading File_1.zip and encoding it using a Reed-Solomon encoder. <br>\n",
    "2- Generating noisy reads from the encoded file. <br>\n",
    "3- Clustering the reads using either the Trivial clustering or the Locality-Sensitive Hashing (LSH) method. <br>\n",
    "4- Generating candidates from the clusters by performing multiple sequence alignment and majority voting. <br>\n",
    "5- Putting the candidates into a Reed-Solomon decoder and recovering the oiginal data. <br>\n",
    "*Encoding note: <br>\n",
    "Note that n, k, N, K, nus, and numblocks (this one is printed out when the following function is called) will be needed for decoding. <br> <br>\n",
    "Real synthesized data: <br>\n",
    "1- Load files File1_ODNA.txt and I16_S2_R1_001.fastq which contain the original sequences and the noisy reads, respectively. <br>\n",
    "2- Clustering the reads using either the Trivial clustering or the Locality-Sensitive Hashing (LSH) method. <br>\n",
    "3- Generating candidates from the clusters by performing multiple sequence alignment and majority voting. <br>\n",
    "4- Putting the candidates into a Reed-Solomon decoder and recovering the oiginal data. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(infile):\n",
    "    stat = sp.Popen([\"./simulate/texttodna\", \"--n=16383\", \"--k=10977\", \"--N=20\", \"--K=18\", \"--nuss=6\", \"--l=4\", \\\n",
    "                     \"--encode\", infile,\"--output=./data/data_encoded.txt\"],stdout = sp.PIPE)\n",
    "    for line in stat.stdout.read().splitlines():\n",
    "        print(line)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_reads(infile,p,n):\n",
    "    # infile: encoded file\n",
    "    # p: deletion probabilty per character (this increases per character index to mimick the synthesis procedure)\n",
    "    # n: number of noisy reads per original sequence\n",
    "    \n",
    "    orig_seqs = []\n",
    "    f = open(infile,\"r\")\n",
    "    for seq in f.read().splitlines():\n",
    "        orig_seqs += [seq]\n",
    "    \n",
    "    \n",
    "    rand_replace = lambda c,d: c if random() > p*d else '' # this function induces deletions\n",
    "    \n",
    "    population = []\n",
    "    for seq in orig_seqs:\n",
    "        for i in range(n):\n",
    "            s = ''.join([rand_replace(c,j) for j,c in enumerate(seq)])\n",
    "            population += [s]\n",
    "    shuffle(population)\n",
    "    return orig_seqs,population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMULATE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'fieldpower: lookup tables generated.. '\n",
      "b'fieldpower: lookup tables generated.. '\n",
      "b'--------------------------------'\n",
      "b'redundancy outer code: 0.492484 (= (n-k)/k)'\n",
      "b'redundancy inner code: 0.111111 (= (N-K)/K)'\n",
      "b'--------------------------------'\n",
      "b'start encoding..'\n",
      "b'infile:  ./data/File_1.zip'\n",
      "b'outfile: ./data/data_encoded.txt'\n",
      "b'Filesize in Byte: 99103'\n",
      "b'encode block 0'\n",
      "b'\\t encode part  0'\n",
      "b'\\t encode part  1'\n",
      "b'\\t encode part  2'\n",
      "b'\\t encode part  3'\n",
      "b'\\t encode part  4'\n",
      "b'\\t encode part  5'\n",
      "b'encoded block 0 of 1'\n",
      "b'encoded 99103 Bytes to 1 blocks, resulting in 16383 DNA segments of length 20 each.'\n",
      "Encoding time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "if SIMULATE:\n",
    "    ##### Encode\n",
    "    s = time.time()\n",
    "    encode(\"--input=./data/File_1.zip\")\n",
    "    print(\"Encoding time:\",round(time.time()-s,1),\"s\")\n",
    "    ##### Generate noisy reads by inducing deletions (perturbation step)\n",
    "    orig_seqs,reads = generate_noisy_reads(\"./data/data_encoded.txt\",0.0005,6)\n",
    "else:\n",
    "    ##### Load synthesized data\n",
    "    orig_seqs = file_to_list(\"data/File1_ODNA.txt\")\n",
    "    filename = \"data/I16_S2_R1_001.fastq\"\n",
    "    seqs = fastq_to_list(filename)\n",
    "    print(\"all sequences: \", len(seqs))\n",
    "    print(\"all orig sequences: \", len(orig_seqs))\n",
    "    reads = [seq for seq in seqs if len(seq) >= 55 and len(seq)<=70]\n",
    "    print(\"all trimmed sequences: \",  len(reads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "You may choose to work with one of the following clustering methods: <br>\n",
    "1. Trivial clustering: looking at the first \"nbeg\" characters of each sequence to cluster the reads. This option is way faster than the other one, especially if you choose to work with the syntesized dataset. This is important since choosing this option reducing runtime of the following sections. <br>\n",
    "2. Locality-Sensititve Hashing (LSH): clustering the reads based on the computed LSH signatures. If this option is chosen when working on the synthesized dataset, both clusering and alignment stages will be slower than the trivial clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===== assign numbers to shingles of each sequence=====#\n",
    "def kmerDNA(seq,k=3):\n",
    "    kmer = []\n",
    "    for ell in range(len(seq)-k+1):\n",
    "        nstr = seq[ell:ell+k]\n",
    "        index = 0\n",
    "        for j,c in enumerate(nstr):\n",
    "            if c == 'A':\n",
    "                i = 0\n",
    "            elif c == 'C':\n",
    "                i = 1\n",
    "            elif c == 'G':\n",
    "                i = 2\n",
    "            elif c == 'T':\n",
    "                i = 3\n",
    "            else:\n",
    "                index = -1\n",
    "                break\n",
    "            index += i*(4**j)\n",
    "        kmer += [index]\n",
    "    return kmer\n",
    "#=====min-hash object=====#\n",
    "class minhashsig():\n",
    "    # min-hash of k-mers\n",
    "    def __init__(self,m,k):\n",
    "        # m is the number of signatures\n",
    "        self.tables = [np.random.permutation(4**k) for i in range(m)]\n",
    "        self.k = k\n",
    "    def generate_signature(self,seq):\n",
    "        kmer = kmerDNA(seq,self.k)\n",
    "        sig = [ min([table[i] for i in kmer]) for table in self.tables]\n",
    "        return sig\n",
    "#=====pair detection=====#\n",
    "def extract_similar_pairs(sigs,m,k_lsh,ell_lsh,maxsig):\n",
    "    # sigs: minhash signatures\n",
    "    # ell_lsh: number of LSH signatures\n",
    "    # k_lsh: number of MH signatures to be concatenated\n",
    "    # we use generatrs to yield a number of pairs at a time for the sake of memory efficiency\n",
    "    \n",
    "    pairs = set([])\n",
    "    \n",
    "    # generate ell_lsh random indices\n",
    "    for ell in range(ell_lsh):\n",
    "        pair_count = 0\n",
    "        s = time.time()\n",
    "        lshinds = np.random.permutation(m)[:k_lsh]\n",
    "        # generate LSh signatures\n",
    "        lshsigs = []\n",
    "        for sig in sigs:\n",
    "            lshsig = 0\n",
    "            for i,lshind in enumerate(lshinds):\n",
    "                lshsig += sig[lshind]*(maxsig**i)\n",
    "            lshsigs += [lshsig]\n",
    "        d = {}\n",
    "        for ind,sig in enumerate(lshsigs):\n",
    "            if sig in d:\n",
    "                d[sig] += [ind]\n",
    "            else:\n",
    "                d[sig] = [ind]\n",
    "        for candidates in d.values():\n",
    "            cent = set([])\n",
    "            if len(candidates) > 1:\n",
    "                for pair in itertools.combinations(candidates,2):\n",
    "                    cent.add(pair[0])\n",
    "                    if len(cent)==1:\n",
    "                        pairs.add(pair)\n",
    "                    else:\n",
    "                        break\n",
    "                        \n",
    "        yield pairs,ell\n",
    "        pair_count += len(pairs)\n",
    "        pairs = set([])\n",
    "#=====form clusters based on pairs=====#\n",
    "def center_cluster(pairs):\n",
    "    clusters = {}\n",
    "    hold = 0\n",
    "    t_counter = 0\n",
    "    ell_copy = 0\n",
    "    pairsize = 0\n",
    "    while not hold:\n",
    "        \n",
    "        try:\n",
    "            out = next(pairs)\n",
    "            pairs_sort = list(out[0])\n",
    "            ell = out[1]\n",
    "            pairsize += len(pairs_sort)\n",
    "            pairs_sort.sort()\n",
    "            s = time.time()\n",
    "            for (u,v) in pairs_sort:\n",
    "                if u in clusters:\n",
    "                    clusters[u] += [v]\n",
    "        \n",
    "                if v in clusters:\n",
    "                    clusters[v] += [u]\n",
    "        \n",
    "                if v not in clusters and u not in clusters:\n",
    "                    clusters[u] = [v]\n",
    "    \n",
    "        except StopIteration:\n",
    "            hold = 1\n",
    "            print(\"clustering completed\",\"---\",pairsize,\"pairs clustered\")\n",
    "        if ell==ell_copy:\n",
    "            t_counter += time.time()-s\n",
    "        else:\n",
    "            print(\"Clustering time for LSH\",ell_copy,\":\",t_counter,'\\n')\n",
    "            t_counter = time.time()-s\n",
    "            ell_copy = ell\n",
    " \n",
    "    return clusters\n",
    "\n",
    "#=====LSH clustering (main function)=====#\n",
    "def lsh_cluster(seqs,m,k,k_lsh=2,ell_lsh=4):\n",
    "    # This is the main function\n",
    "    maxsig = 4**k\n",
    "    minhash = minhashsig(m,k)\n",
    "    sigs = [minhash.generate_signature(seq[:40]) for seq in seqs]\n",
    "    pairs = extract_similar_pairs(sigs,m,k_lsh,ell_lsh,maxsig)\n",
    "    clusters = center_cluster(pairs)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_nonunique(seqs):\n",
    "    # filter out sequences that appear many times\n",
    "    d = {}\n",
    "    ctr = 0\n",
    "    for i,seq in enumerate(seqs):\n",
    "        if seq in d:\n",
    "            d[seq] += [i]\n",
    "        else:\n",
    "            d[seq] = [i]\n",
    "    import operator\n",
    "    sorted_d = sorted(d.items(), key=operator.itemgetter(1))\n",
    "    sorted_d.reverse()\n",
    "    return d,ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIVIAL = True # when real synthesized data is selected, set this flag to \"True\" (this dramatically reduces the runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 0.1 s\n",
      "16361 number of clusters created.\n"
     ]
    }
   ],
   "source": [
    "if TRIVIAL:\n",
    "    start = time.time()\n",
    "    nbeg=14\n",
    "    d,ctr = filter_nonunique([seq[:nbeg] for seq in reads])\n",
    "    clusters = [d[a] for a in d if len(d[a]) > 3]\n",
    "    end = time.time()\n",
    "    print(\"Runtime:\",round(end-start,1),\"s\")\n",
    "    print(len(clusters),\"number of clusters created.\")\n",
    "    fclusts = clusters.copy()\n",
    "else:\n",
    "    # set up the parameters and call the lsh_cluster function\n",
    "    k_lsh = 4\n",
    "    sim = 0.5\n",
    "    ell_lsh = int(1/(sim**k_lsh))\n",
    "    m,k=50,5\n",
    "    start = time.time()\n",
    "    clusters = lsh_cluster(reads,m,k,k_lsh,ell_lsh)\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Runtime:\",round(end-start,1),\"s\")\n",
    "    print(len(clusters),\"number of clusters created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the clusters (do this step only if you have chosen LSH clustering)\n",
    "In this stage, we use the max_match function to filter the clusters. This function checks the simlarity of each cluster member and its cluster center based on local alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the center of each cluter to the cluster (also removing duplicates from each cluster)\n",
    "clusts = [ [c] + list(set(clusters[c])) for c in clusters if len(clusters[c]) > 3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====max matching=====# \n",
    "def max_match(seq1,seq2):\n",
    "    # This function checks whether seq1 and seq2 are similar or not\n",
    "    # Checking all pairs within a cluster dramatically increases the time complexity, \n",
    "    # so by default, in the next cell, we call this function to only check the pairs\n",
    "    # that one of their members is the cluster center\n",
    "    \n",
    "    alignment,score,start_end_positions \\\n",
    "        = local_pairwise_align_ssw(DNA(seq1) , DNA(seq2) , match_score=2,mismatch_score=-3)\n",
    "    a = str(alignment[0])\n",
    "    b = str(alignment[1])\n",
    "    ctr = 0\n",
    "    for i,j in zip(a,b):\n",
    "        if i==j:\n",
    "            ctr += 1\n",
    "    return ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% 0.0 of the clusters are filtered.\n",
      "% 3.88 of the clusters are filtered.\n",
      "% 7.75 of the clusters are filtered.\n",
      "% 11.63 of the clusters are filtered.\n",
      "% 15.5 of the clusters are filtered.\n",
      "% 19.38 of the clusters are filtered.\n",
      "% 23.25 of the clusters are filtered.\n",
      "% 27.13 of the clusters are filtered.\n",
      "% 31.0 of the clusters are filtered.\n",
      "% 34.88 of the clusters are filtered.\n",
      "% 38.75 of the clusters are filtered.\n",
      "% 42.63 of the clusters are filtered.\n",
      "% 46.5 of the clusters are filtered.\n",
      "% 50.38 of the clusters are filtered.\n",
      "% 54.26 of the clusters are filtered.\n",
      "% 58.13 of the clusters are filtered.\n",
      "% 62.01 of the clusters are filtered.\n",
      "% 65.88 of the clusters are filtered.\n",
      "% 69.76 of the clusters are filtered.\n",
      "% 73.63 of the clusters are filtered.\n",
      "% 77.51 of the clusters are filtered.\n",
      "% 81.38 of the clusters are filtered.\n",
      "% 85.26 of the clusters are filtered.\n",
      "% 89.13 of the clusters are filtered.\n",
      "% 93.01 of the clusters are filtered.\n",
      "% 96.88 of the clusters are filtered.\n",
      "filtering time for 25804 clusters: 107.64 s\n"
     ]
    }
   ],
   "source": [
    "th = 35 # filtering threshold\n",
    "\n",
    "k = len(clusts)\n",
    "s = time.time()\n",
    "fclusts = []\n",
    "for i,c in enumerate(clusts):\n",
    "    cent = reads[c[0]]\n",
    "    cc = [c[0]]\n",
    "    for e in c[1:]:\n",
    "        score = max_match(cent,reads[e])\n",
    "        if score >= th:\n",
    "            cc += [e]\n",
    "    fclusts += [cc]\n",
    "    if i%1000 == 0:\n",
    "        print(\"%\",round(i*100/len(clusts),2),\"of the clusters are filtered.\")\n",
    "print(\"filtering time for\",k,\"clusters:\",round(time.time()-s,2),\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning the clusters\n",
    "As for the alignment stage, there are a number of options differing in accuracy and speed. For the sake of having the highest accuracy, we chose the Muscle function which is the most accurate multiple-alignment function among all python built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_alignment_muscle(cluster,out=False):\n",
    "    # write cluster to file\n",
    "    file = open(\"clm.fasta\",\"w\") \n",
    "    for i,c in enumerate(cluster):\n",
    "        file.write(\">S%d\\n\" % i)\n",
    "        file.write(c)\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    muscle_exe = r\"~/muscle3.8.31_i86linux64\" # assuming you've already put this in the main directory\n",
    "    output_alignment = \"clmout.fasta\"\n",
    "    muscle_cline = MuscleCommandline(muscle_exe, input=\"clm.fasta\", out=output_alignment)\n",
    "    stdout, stderr = muscle_cline()\n",
    "    msa = AlignIO.read(output_alignment, \"fasta\")\n",
    "    if out:\n",
    "        print(msa)\n",
    "    alignedcluster = []\n",
    "    for i in msa:\n",
    "        alignedcluster += [i.seq]\n",
    "    return alignedcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresults = []\n",
    "def align_clusters(clusters,masize = 15):\n",
    "    ### align clusters, generate candidates\n",
    "    for i, clusterinds in enumerate(clusters):\n",
    "        cluster = [reads[i] for i in clusterinds]\n",
    "        if len(cluster) < 3:\n",
    "            continue\n",
    "        if len(cluster) > masize:\n",
    "            for j in range(5):\n",
    "                shuffle(cluster)\n",
    "                ma = multiple_alignment_muscle(cluster[:masize])\n",
    "                fresults.append(ma)\n",
    "        else:\n",
    "            ma = multiple_alignment_muscle(cluster[:masize])\n",
    "            fresults.append(ma)\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            print(\"%\",round(i*100/len(clusters),2),\"of the clusters are aligned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% 0.0 of the clusters are aligned.\n",
      "% 6.11 of the clusters are aligned.\n",
      "% 12.22 of the clusters are aligned.\n",
      "% 18.34 of the clusters are aligned.\n",
      "% 24.45 of the clusters are aligned.\n",
      "% 30.56 of the clusters are aligned.\n",
      "% 36.67 of the clusters are aligned.\n",
      "% 42.78 of the clusters are aligned.\n",
      "% 48.9 of the clusters are aligned.\n",
      "% 55.01 of the clusters are aligned.\n",
      "% 61.12 of the clusters are aligned.\n",
      "% 67.23 of the clusters are aligned.\n",
      "% 73.35 of the clusters are aligned.\n",
      "% 79.46 of the clusters are aligned.\n",
      "% 85.57 of the clusters are aligned.\n",
      "% 91.68 of the clusters are aligned.\n",
      "% 97.79 of the clusters are aligned.\n",
      "Alignment Runtime: 273.77 s\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "align_clusters(fclusts,15)\n",
    "print(\"Alignment Runtime:\",round(time.time()-s,2),\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority merging\n",
    "At this stage, we merge each aligned cluster by putting up a voting for each position within the aligned sequences. Then, the fraction of all original sequences recovered is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the fraction of origignal squences recovered given a number of candidates\n",
    "def fraction_recovered(candidates,orig_seqs):\n",
    "    d = {}\n",
    "    for seq in orig_seqs:\n",
    "        d[seq] = 0\n",
    "    for cand in candidates:\n",
    "        if cand in d:\n",
    "            d[cand] += 1\n",
    "    av = sum([ d[seq]>0 for seq in d]) / len(d)\n",
    "    print(\"Fraction of recovered sequences: \", av )\n",
    "    if av>0:\n",
    "        print(\"Fraction of recovered sequences: \", sum([ d[seq] for seq in d]) / len(d) / av )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_merge(reads,weight = 0.4):\n",
    "    # assume reads have the same length\n",
    "    res = \"\"\n",
    "    for i in range(len(reads[0])):\n",
    "        counts = {'A':0,'C':0,'G':0,'T':0,'-':0,'N':0}\n",
    "        for j in range(len(reads)):\n",
    "            counts[reads[j][i]] +=1\n",
    "        counts['-'] *= weight\n",
    "        mv = max(counts.items(), key=operator.itemgetter(1))[0]\n",
    "        if mv != '-':\n",
    "            res += mv\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of recovered sequences:  0.9833363852774217\n",
      "Fraction of recovered sequences:  1.0\n"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for ma in fresults:\n",
    "    candidates.append(majority_merge(ma,weight=0.5))\n",
    "fraction_recovered( [seq[:60] for seq in candidates] , orig_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./data/data_drawnseg.txt\",\"w\")\n",
    "for i,seq in enumerate(candidates):\n",
    "    if len(seq) >= 60:\n",
    "        f.write(seq[:60])\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(infile):\n",
    "    stat = sp.Popen([\"./simulate/texttodna\", \"--decode\",  \"--n=16383\", \"--k=10977\", \"--N=20\", \"--K=18\", \"--nuss=6\", \"--l=4\", \\\n",
    "                    \"--numblocks=1\", infile, \"--output=./data/data_rec.zip\"],stdout=sp.PIPE)\n",
    "    for line in stat.stdout.read().splitlines():\n",
    "        print(line)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'fieldpower: lookup tables generated.. '\n",
      "b'fieldpower: lookup tables generated.. '\n",
      "b'--------------------------------'\n",
      "b'redundancy outer code: 0.492484 (= (n-k)/k)'\n",
      "b'redundancy inner code: 0.111111 (= (N-K)/K)'\n",
      "b'--------------------------------'\n",
      "b'infile:  ./data/data_drawnseg.txt'\n",
      "b'outfile: ./data/data_rec.zip'\n",
      "b'numblocks: 1'\n",
      "b'assume text file as input format'\n",
      "b'start decode..'\n",
      "b'number of reads: 16184'\n",
      "b'inner code: 0.00451063 errors on average corrected per sequence'\n",
      "b'272 many erasures in block 0 of length 16383'\n",
      "b'remove random sequence'\n",
      "b'decoding block 0'\n",
      "b'\\tpart 0/6'\n",
      "b'\\tpart 1/6'\n",
      "b'\\tpart 2/6'\n",
      "b'\\tpart 3/6'\n",
      "b'\\tpart 4/6'\n",
      "b'\\tpart 5/6'\n",
      "b'\\touter code: 272 errasures, 1 errors corrected'\n",
      "b'Wrote file of length 99103 Bytes'\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "decode(\"--input=./data/data_drawnseg.txt\")\n",
    "runtime = round(time.time()-s,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding time: 70.0 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoding time:\",runtime,\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Let's first extract the decoded file (data_rec.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('./data/data_rec.zip', 'r') as zipObj:\n",
    "   # Extract zip file contents to the \"decoded\" directory\n",
    "   zipObj.extractall(path = \"./data/decoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the files in File_1.zip and the decoded files located in the 'decoded' folder, we see that the original files are completely recovered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
